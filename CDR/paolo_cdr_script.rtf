{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11760\viewh19360\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs28 \cf0 Object detection\
\
For the object detection task, so to detect face masks, we will use Darknet framework in conjunction with the YOLOv4-tiny object detection system (where YOLO stands for \'93you only look once\'94). In order to obtain a model trained for our particular recognition task, we will use a dataset of images showing both people wearing masks and people not wearing masks. Unlike other detectors YOLO applies a single neural network to the whole image, dividing it into regions and predicting bounding box and probabilities for each region. This has several advantages, among which a faster inference time as it can be seen from this graph. In fact, we can see that the mAP@50% confidence for the MS COCO dataset, which is used to benchmark detectors (and has 80 classes), is 40.2% for YOLOv4-tiny. That could sound low but no other model can obtain the same performance in terms of FPS with the same mAP. For our particular task, which is not as hard as the MS COCO benchmark, as it only has 2 classes, we will have the input resolution set at 800x800. As mentioned before by my colleague, we expect a mAP@50% confidence in the neighborhood of 85 to 90%.\
\
Data collection\
\
In order to provide insights on the behavior of people in the surveilled areas, we are going to save each detection in a mongodb database as a new document. Each document will be assigned a random ID and we will record wether the person was wearing a face mask or not, as well as the timestamp of the detection. Considering that dealing with footage of people means dealing with sensitive data and having to follow strict privacy regulations, as well as having an adequate amount of storage space, we won\'92t be saving any of that, thus guaranteeing data anonymity. In any case there was a ruling in Italy saying that the users need to be informed of the presence of a camera even if the images are not saved, so it would be still necessary to show a sign signaling the camera activity.\
\
Data visualization\
\
The previously saved data will be shown to the system managers in a web UI, which will be hosted on a CherryPy web server. The website will use Angular as a JS framework which will enable us to more easily interact with the database and show meaningful graphs regarding the collected data. We will be using Apexcharts, which is an open-source charting library, for showing the graphs. For example here we can see a graph of the total number of detections and another one with the percentage of people wearing and not wearing masks among the detected ones.\
\
Development tools\
\
As mentioned before, we are now using a Jetson Nano developer kit, which is a single board computer with limited resources. In order to maximize the usage of such resources for our scripts only, we have decided to run the computer in headless mode, i.e. without using a directly attached screen. Not loading the GUI saves RAM which we will need to for the object tracking and detection. Since we do not have a directly attached screen but we still need to visualize the detection output while we are developing our scripts, we are now using some additional tools which will be disabled after the development has been completed. In particular, to view to processed images, we are encoding the video feed in real-time with opencv as an m-jpeg stream which we are then serving on the LAN with Flask. In this way we can see the video feed directly in a browser, while on the same LAN as the Jetson. As I mentioned before, as these tools allow to view a real-time stream of the images, we will disable them once the system is ready.}